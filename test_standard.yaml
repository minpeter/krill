# Test configuration for standard preprocessing
vocab_size: 32000
hub_tokenizer_id: minpeter/webtext-tokenizer-32k

# krill preprocess
sequence_len: 512
dataset_prepared_path: ./test_artifacts/standard_test
dataset_prepared_min_length: 50

# Memory efficiency settings (disabled)
preprocess_memory_efficient: false

datasets:
  - path: HAERAE-HUB/KOREAN-WEBTEXT
    split: train[:1000]  # Small subset for testing
    text_column: text

# krill train (not used for preprocessing test)
hub_model_id: test/standard
output_dir: ./test_artifacts/models/test

num_epochs: 1
learning_rate: 1e-3
weight_decay: 0.01
optimizer: muon
muon_implementation: moonlight

micro_batch_size: 1
gradient_accumulation_steps: 1

model_config_name: pico