# Example configuration for memory-efficient preprocessing
# This demonstrates the new memory efficiency features

# Tokenizer configuration
vocab_size: 48000
hub_tokenizer_id: pretraining/fw2-edu-kr-tokenizer-48k

# Preprocessing configuration
sequence_len: 2048
dataset_prepared_path: ./artifacts/dataset_memory_efficient
dataset_prepared_min_length: 100

# Memory efficiency settings (simplified approach)
preprocess_memory_efficient: true # Enable memory-efficient mode
preprocess_chunk_size: 1000 # Batch size for tokenization (reduced from 5000)
preprocess_save_shard_size: 50MB # Smaller shards to reduce memory spikes during save

# Dataset configuration
datasets:
  - path: minpeter/fineweb-2-edu-korean
    split: train[:50_000]  # Reduced for testing
    text_column: text

# Training configuration (not used during preprocessing)
hub_model_id: your/model/id
output_dir: ./artifacts/models/your_model

num_epochs: 1
learning_rate: 3e-4
weight_decay: 0.01
optimizer: muon
muon_implementation: moonlight

micro_batch_size: 4
gradient_accumulation_steps: 8

model_config_name: small
