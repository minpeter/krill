# Example configuration for memory-efficient preprocessing
# This demonstrates the new memory efficiency features

# Tokenizer configuration
vocab_size: 32000
hub_tokenizer_id: minpeter/webtext-tokenizer-32k

# Preprocessing configuration
sequence_len: 2048
dataset_prepared_path: ./artifacts/dataset_memory_efficient
dataset_prepared_min_length: 100

# Memory efficiency settings (NEW FEATURES)
preprocess_memory_efficient: true        # Enable memory-efficient mode
preprocess_chunk_size: 5000              # Process in chunks of 5000 samples
preprocess_dedup_cache_dir: ./artifacts/cache/dedup # Persistent deduplication cache
preprocess_save_shard_size: 100MB        # Reduce memory spike during save with smaller shards

# Dataset configuration
datasets:
  - path: minpeter/fineweb-2-edu-korean
    split: train[:100_000]
    text_column: text

# Training configuration (not used during preprocessing)
hub_model_id: your/model/id
output_dir: ./artifacts/models/your_model

num_epochs: 1
learning_rate: 3e-4
weight_decay: 0.01
optimizer: muon
muon_implementation: moonlight

micro_batch_size: 4
gradient_accumulation_steps: 8

model_config_name: small