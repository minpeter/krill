# Test configuration for memory-efficient preprocessing
vocab_size: 32000
hub_tokenizer_id: minpeter/webtext-tokenizer-32k

# krill preprocess
sequence_len: 512
dataset_prepared_path: ./test_artifacts/memory_efficient_test
dataset_prepared_min_length: 50

# Memory efficiency settings
preprocess_memory_efficient: true
preprocess_chunk_size: 1000
preprocess_dedup_cache_dir: ./test_artifacts/dedup_cache

datasets:
  - path: HAERAE-HUB/KOREAN-WEBTEXT
    split: train[:1000]  # Small subset for testing
    text_column: text

# krill train (not used for preprocessing test)
hub_model_id: test/memory-efficient
output_dir: ./test_artifacts/models/test

num_epochs: 1
learning_rate: 1e-3
weight_decay: 0.01
optimizer: muon
muon_implementation: moonlight

micro_batch_size: 1
gradient_accumulation_steps: 1

model_config_name: pico